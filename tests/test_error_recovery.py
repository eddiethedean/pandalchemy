"""Tests for error recovery and rollback scenarios."""

import pandas as pd
import pytest
from sqlalchemy import create_engine

from pandalchemy import DataBase
from pandalchemy.exceptions import DataValidationError, SchemaError


def test_multi_table_push_partial_failure_rollback(tmp_path):
    """Test that partial failure in multi-table push rolls back all changes."""
    db_path = tmp_path / "rollback.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    # Create two tables
    users = pd.DataFrame({
        'id': [1, 2],
        'name': ['Alice', 'Bob']
    })
    db.create_table('users', users, primary_key='id')

    orders = pd.DataFrame({
        'id': [1],
        'user_id': [1],
        'total': [100.0]
    })
    db.create_table('orders', orders, primary_key='id')

    # Make changes to both tables
    db['users'].data.update_row(1, {'name': 'Alicia'})

    # This will cause validation error (dropping PK)
    db['orders'].data._data = db['orders'].data._data.reset_index()
    db['orders'].data.drop_column_safe('id')

    # Try to push - should fail and rollback
    with pytest.raises(SchemaError):
        db.push()

    # Verify rollback - pull fresh data
    db = DataBase(engine)
    # Users table should NOT have the update (rolled back)
    assert db['users'].data.get_row(1)['name'] == 'Alice'


def test_validation_failure_prevents_push(tmp_path):
    """Test that validation errors prevent push."""
    db_path = tmp_path / "validation.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    users = pd.DataFrame({
        'id': [1, 2, 3],
        'name': ['Alice', 'Bob', 'Charlie']
    })
    db.create_table('users', users, primary_key='id')

    # Try to add duplicate PK
    try:
        db['users'].data.add_row({'id': 2, 'name': 'Duplicate'})
    except DataValidationError:
        pass  # Expected

    # Verify nothing was changed
    assert not db['users'].data.has_changes()


def test_null_pk_validation_failure(tmp_path):
    """Test that null in PK causes validation failure."""
    db_path = tmp_path / "null_pk.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    users = pd.DataFrame({
        'id': [1, 2],
        'name': ['Alice', 'Bob']
    })
    db.create_table('users', users, primary_key='id')

    # Try to add row with null PK
    with pytest.raises(DataValidationError):
        db['users'].data.add_row({'id': None, 'name': 'NullID'})


def test_recovery_pattern_fix_and_retry(tmp_path):
    """Test error recovery pattern: catch error, fix data, retry."""
    db_path = tmp_path / "retry.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    users = pd.DataFrame({
        'id': [1, 2],
        'name': ['Alice', 'Bob']
    })
    db.create_table('users', users, primary_key='id')

    # Attempt with duplicate PK
    try:
        db['users'].data.add_row({'id': 2, 'name': 'Duplicate'})
        db.push()
        raise AssertionError("Should have raised error")
    except DataValidationError as e:
        # Catch error, fix by using different ID
        print(f"Caught error: {e}")

        # Note: The failed add_row actually did add to tracker
        # So we need to start fresh
        db = DataBase(engine)

        # Now add with correct ID
        db['users'].data.add_row({'id': 3, 'name': 'Charlie'})
        db.push()

        # Verify success
        db.pull()
        assert db['users'].data.row_exists(3)


def test_incremental_push_pattern(tmp_path):
    """Test incremental push pattern for large changes."""
    db_path = tmp_path / "incremental.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    # Create table
    data = pd.DataFrame({
        'id': [1],
        'value': [0]
    })
    db.create_table('data', data, primary_key='id')

    # Make many changes in batches
    batch_size = 100
    total_rows = 500

    for batch_start in range(2, total_rows + 1, batch_size):
        batch_end = min(batch_start + batch_size, total_rows + 1)

        # Reload for each batch
        db = DataBase(engine)

        # Add rows in this batch
        for i in range(batch_start, batch_end):
            db['data'].data.add_row({'id': i, 'value': i * 10})

        # Push incrementally
        db.push()

    # Verify all added
    db = DataBase(engine)
    assert len(db['data']) == total_rows
    assert db['data'].data.row_exists(total_rows)


def test_validation_all_error_types(tmp_path):
    """Test that all validation error types are caught."""
    db_path = tmp_path / "all_errors.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    # Create table
    users = pd.DataFrame({
        'id': [1, 2],
        'name': ['Alice', 'Bob']
    })
    db.create_table('users', users, primary_key='id')

    # Test 1: Duplicate PK
    with pytest.raises(DataValidationError, match="already exists"):
        db['users'].data.add_row({'id': 1, 'name': 'Dup'})

    # Test 2: Null PK
    with pytest.raises(DataValidationError, match="null|missing"):
        db['users'].data.add_row({'id': None, 'name': 'Null'})

    # Test 3: Missing PK
    with pytest.raises(DataValidationError, match="missing|required"):
        db['users'].data.add_row({'name': 'No ID'})

    # Test 4: Trying to update PK
    with pytest.raises(DataValidationError, match="Cannot update primary key"):
        db['users'].data.update_row(1, {'id': 999})


def test_complex_five_table_failure_rollback(tmp_path):
    """Test rollback with 5 tables where table 3 fails."""
    db_path = tmp_path / "five_tables.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    # Create 5 tables
    for i in range(1, 6):
        data = pd.DataFrame({
            'id': [1, 2],
            'value': [i * 10, i * 20],
            'name': [f'Table{i}_A', f'Table{i}_B']
        })
        db.create_table(f'table{i}', data, primary_key='id')

    # Make changes to all tables
    for i in range(1, 6):
        db[f'table{i}'].data.update_row(1, {'value': 999})

    # Corrupt table3 (drop PK)
    db['table3'].data._data = db['table3'].data._data.reset_index()
    db['table3'].data.drop_column_safe('id')

    # Try to push - should fail due to table3 validation
    with pytest.raises(SchemaError):
        db.push()

    # Note: In current implementation, tables are pushed sequentially
    # Tables before the failure may have been committed
    # This is a known limitation - for atomic multi-table updates,
    # validate all tables first, then push
    db = DataBase(engine)
    # The test documents the current behavior (not necessarily ideal)
    # Table 1 may have been pushed before table3 failed
    assert db['table1'].data.row_exists(1)  # Just verify it exists


def test_checkpoint_and_resume_pattern(tmp_path):
    """Test checkpoint and resume pattern for long operations."""
    db_path = tmp_path / "checkpoint.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    # Create processing table
    jobs = pd.DataFrame({
        'id': range(1, 101),
        'status': ['pending'] * 100,
        'result': [None] * 100
    })
    db.create_table('jobs', jobs, primary_key='id')

    # Simulate processing with checkpoints
    batch_size = 10
    last_processed = 0

    for batch_start in range(1, 101, batch_size):
        batch_end = min(batch_start + batch_size, 101)

        # Reload
        db = DataBase(engine)

        # Process batch
        for job_id in range(batch_start, batch_end):
            # Simulate processing
            db['jobs'].data.update_row(job_id, {
                'status': 'completed',
                'result': f'Result for job {job_id}'
            })
            last_processed = job_id

        # Checkpoint (push)
        try:
            db.push()
        except Exception as e:
            print(f"Error at job {last_processed}: {e}")
            # In real scenario, could resume from last_processed + 1
            break

    # Verify checkpoint pattern worked
    db = DataBase(engine)
    completed = len(db['jobs'].data[db['jobs'].data['status'] == 'completed'])
    assert completed == 100


def test_handling_constraint_violation_during_push(tmp_path):
    """Test handling constraint violations during push."""
    db_path = tmp_path / "constraint.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    users = pd.DataFrame({
        'id': [1, 2, 3],
        'email': ['alice@test.com', 'bob@test.com', 'charlie@test.com']
    })
    db.create_table('users', users, primary_key='id')

    # Try to add duplicate (should be caught by validation)
    with pytest.raises(DataValidationError):
        db['users'].data.add_row({'id': 1, 'email': 'dup@test.com'})

    # Verify no changes made
    db.pull()
    assert len(db['users']) == 3


def test_recovery_from_corrupted_tracker_state(tmp_path):
    """Test recovering from potentially corrupted tracker state."""
    db_path = tmp_path / "recovery.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    users = pd.DataFrame({
        'id': [1, 2],
        'name': ['Alice', 'Bob']
    })
    db.create_table('users', users, primary_key='id')

    # Make changes
    db['users'].data.update_row(1, {'name': 'Alicia'})

    # Simulate issue - don't push
    # In real scenario, might exit program here

    # Recovery: create new DataBase instance
    db = DataBase(engine)

    # Fresh state (changes lost, but database is consistent)
    assert db['users'].data.get_row(1)['name'] == 'Alice'

    # Make new changes
    db['users'].data.update_row(1, {'name': 'Alicia'})
    db.push()

    # Verify recovery successful
    db.pull()
    assert db['users'].data.get_row(1)['name'] == 'Alicia'


def test_partial_batch_failure_isolation(tmp_path):
    """Test that failure in one batch doesn't affect independent batches."""
    db_path = tmp_path / "batch_isolation.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    users = pd.DataFrame({
        'id': [1, 2, 3],
        'name': ['Alice', 'Bob', 'Charlie']
    })
    db.create_table('users', users, primary_key='id')

    # Batch 1: Valid changes
    db['users'].data.update_row(1, {'name': 'Alicia'})
    db.push()

    # Verify batch 1 succeeded
    db.pull()
    assert db['users'].data.get_row(1)['name'] == 'Alicia'

    # Batch 2: Invalid changes
    try:
        db['users'].data.add_row({'id': 2, 'name': 'Duplicate'})
        db.push()
    except DataValidationError:
        pass  # Expected

    # Batch 1 should still be persisted
    db = DataBase(engine)
    assert db['users'].data.get_row(1)['name'] == 'Alicia'


def test_data_validation_all_types_before_push(tmp_path):
    """Test that pandas/pandalchemy prevent invalid data manipulations."""
    from pandalchemy.exceptions import SchemaError

    db_path = tmp_path / "comprehensive_validation.db"
    engine = create_engine(f"sqlite:///{db_path}")
    db = DataBase(engine)

    # Composite PK table
    memberships = pd.DataFrame({
        'user_id': [1, 2],
        'org_id': ['org1', 'org2'],
        'role': ['admin', 'user']
    })
    db.create_table('memberships', memberships, primary_key=['user_id', 'org_id'])

    # Valid data should pass
    errors = db['memberships'].data.validate_data()
    assert errors == []

    # Test 1: Pandas prevents duplicate column names at DataFrame level
    # This will raise ValueError from pandas itself
    with pytest.raises(ValueError, match="Length mismatch"):
        db['memberships'].data._data.columns = ['role', 'role']

    # Reset
    db = DataBase(engine)

    # Test 2: Dropping PK column prevents push
    db['memberships'].data._data = db['memberships'].data._data.reset_index()
    db['memberships'].data.drop_column_safe('user_id')

    # Should raise SchemaError when trying to push
    with pytest.raises(SchemaError, match="Primary key column"):
        db.push()

